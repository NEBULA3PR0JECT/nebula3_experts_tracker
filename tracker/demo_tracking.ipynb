{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 12:02:29.410604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-14 12:02:29.418896: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ec2-user/miniconda3/envs/tflow/lib/python3.8/site-packages/cv2/../../lib64:/lib:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/lib:/opt/amazon/efa/lib64:/usr/local/mpi/lib:/lib:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/opt/amazon/openmpi/lib64:/usr/local/cuda/lib:/opt/amazon/efa/lib64:/usr/local/mpi/lib:\n",
      "2022-03-14 12:02:29.420256: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-03-14 12:02:29.421710: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 12:03:12.822920: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: -3091 } dim { size: -3092 } dim { size: -3093 } dim { size: 1088 } } } inputs { dtype: DT_FLOAT shape { dim { size: -108 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -108 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 17 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"111\" frequency: 2300 num_cores: 4 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 262144 l3_cache_size: 47185920 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -108 } dim { size: 17 } dim { size: 17 } dim { size: 1088 } } }\n"
     ]
    }
   ],
   "source": [
    "import autotracker as at\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# create a default detection model according to conda env.\n",
    "model = at.detection_utils.VideoPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# configure an experiment\n",
    "experiment_config  = dict(\n",
    "    detect_every=10,          # use detection model every `detect_every` frames (starting from 0) \n",
    "    merge_iou_threshold=0.5,  # required IOU score \n",
    "    tracker_type=at.tracking_utils.TRACKER_TYPE_KCF,  # trecker algorithm (KCF or CSRT)\n",
    "    refresh_on_detect=False   # if True, any object that isn't found by the model is removed\n",
    ")\n",
    "\n",
    "# choose movie. save in appropriate experiment fiile\n",
    "input_path = '/movies/actioncliptrain00188.avi'\n",
    "output_path = os.path.splitext(os.path.basename(input_path))[0] + f'__{\",\".join(f\"{k}={v}\" for k, v in experiment_config.items())}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/demo_tracking.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2228475055292d6563322d6e6577227d/home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/demo_tracking.ipynb#ch0000002vscode-remote?line=3'>4</a>\u001b[0m tracking_data \u001b[39m=\u001b[39m at\u001b[39m.\u001b[39mtracking_utils\u001b[39m.\u001b[39mMultiTracker\u001b[39m.\u001b[39mtrack_video_objects(input_path,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2228475055292d6563322d6e6577227d/home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/demo_tracking.ipynb#ch0000002vscode-remote?line=4'>5</a>\u001b[0m                                                                    model,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2228475055292d6563322d6e6577227d/home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/demo_tracking.ipynb#ch0000002vscode-remote?line=5'>6</a>\u001b[0m                                                                    \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mexperiment_config)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2228475055292d6563322d6e6577227d/home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/demo_tracking.ipynb#ch0000002vscode-remote?line=6'>7</a>\u001b[0m \u001b[39m# annotate video when tracking is complete.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2228475055292d6563322d6e6577227d/home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/demo_tracking.ipynb#ch0000002vscode-remote?line=7'>8</a>\u001b[0m TrackerAnnotator()\u001b[39m.\u001b[39;49mannotate_video(input_path, tracking_data, output_path)\n",
      "File \u001b[0;32m/home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/TrackerAnnotator.py:24\u001b[0m, in \u001b[0;36mTrackerAnnotator.annotate_video\u001b[0;34m(self, video_path, annotations, output_path, video_fps, show_pbar)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/TrackerAnnotator.py?line=20'>21</a>\u001b[0m         anno_fromatted[frame_id][obj_id][\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m obj_data[\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='file:///home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/TrackerAnnotator.py?line=21'>22</a>\u001b[0m annotations \u001b[39m=\u001b[39m anno_fromatted\n\u001b[0;32m---> <a href='file:///home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/TrackerAnnotator.py?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mannotate_video(video_path, annotations, output_path, video_fps\u001b[39m=\u001b[39;49mvideo_fps, show_pbar\u001b[39m=\u001b[39;49mshow_pbar)\n",
      "File \u001b[0;32m/home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/autotracker/tracking/../../../common/BaseAnnotator.py:47\u001b[0m, in \u001b[0;36mBaseAnnotator.annotate_video\u001b[0;34m(self, video_path, annotations, output_path, video_fps, show_pbar)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/autotracker/tracking/../../../common/BaseAnnotator.py?line=43'>44</a>\u001b[0m     ds \u001b[39m=\u001b[39m VideoFile(video_path)\n\u001b[1;32m     <a href='file:///home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/autotracker/tracking/../../../common/BaseAnnotator.py?line=45'>46</a>\u001b[0m \u001b[39m# create video writer object\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/autotracker/tracking/../../../common/BaseAnnotator.py?line=46'>47</a>\u001b[0m height, width, _ \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(ds))\u001b[39m.\u001b[39mshape\n\u001b[1;32m     <a href='file:///home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/autotracker/tracking/../../../common/BaseAnnotator.py?line=47'>48</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(output_path)), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# create path if necessary.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/autotracker/tracking/../../../common/BaseAnnotator.py?line=48'>49</a>\u001b[0m output_file \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mVideoWriter(\n\u001b[1;32m     <a href='file:///home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/autotracker/tracking/../../../common/BaseAnnotator.py?line=49'>50</a>\u001b[0m     filename\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00moutput_path\u001b[39m}\u001b[39;00m\u001b[39m.mp4\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='file:///home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/autotracker/tracking/../../../common/BaseAnnotator.py?line=50'>51</a>\u001b[0m     \u001b[39m# some installation of opencv may not support x264 (due to its license),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/autotracker/tracking/../../../common/BaseAnnotator.py?line=55'>56</a>\u001b[0m     isColor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ilan/git/NEBULA2-latest/NEBULA2/experts/tracker/autotracker/tracking/../../../common/BaseAnnotator.py?line=56'>57</a>\u001b[0m )\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from TrackerAnnotator import TrackerAnnotator\n",
    "\n",
    "# do tracking with experiment config.\n",
    "tracking_data = at.tracking_utils.MultiTracker.track_video_objects(input_path,\n",
    "                                                                   model,\n",
    "                                                                   **experiment_config)\n",
    "# annotate video when tracking is complete.\n",
    "TrackerAnnotator().annotate_video(input_path, tracking_data, output_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1eed08526ee63cda4a8ff9bda89da76bc83324dd7baec5374bda665ca756bd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('detectron': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
